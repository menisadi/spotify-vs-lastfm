# Spotify Wrapped Was Off—But By How Much? A Statistical Deep Dive

Every December, part of our social media feeds turn into a homogeneity of bright, bold graphics. Spotify Wrapped drops, and suddenly everyone is defining their identity by being in the "top 0.5% of Taylor Swift listeners".

But for those of us who track our data obsessively, there’s often a nagging feeling when scrolling through that glossy slideshow: *This feels wrong.*

Did I really listen to that generic Lo-Fi Beats track more than my favorite indie rock album? Or is the "Wrapped" algorithm weighing recency bias or skip-rates in a way that distorts the raw play count?

As we approach the next Wrapped season, I decided to stop relying on vibes and start relying on vectors. I dug up my data from last year to answer a simple question: **If I treat my listening history as a ranked list, what is the distance between Spotify’s marketing campaign and the ground truth?**

## The Ground Truth: Last.fm vs. The "Black Box"

To measure error, you need a reference point. For music tracking, the gold standard remains Last.fm. It scrobbles everything—not just Spotify, but local files and other streams—providing a timestamped, raw log of consumption.

For this experiment, I pulled two datasets:

1.  **The Challenger:** My "Top Songs" playlist generated by Spotify Wrapped.
2.  **The Champion:** My calculated top 100 tracks from Last.fm for the same period.

### Data Cleaning (The Boring but Necessary Part)

As any data scientist knows, you can’t compare strings without sanitizing them first. `feat.` vs `ft.`, remastered tags, and capitalization differences will ruin any set intersection operation.

I used a simple standardization pipeline using Python’s `pandas` and `string.capwords` to normalize the inputs:

```python
def standardize_title(title: str) -> str:
    """Convert a track title to standard title case format."""
    return capwords(title.strip())

def filter_alphanumeric(input_string):
    return "".join(char for char in input_string if char.isalnum())
```

Once cleaned, we are left with two ordered lists of length $N$. Now comes the fun part: How do we mathematically quantify the "disagreement" between two playlists?

## Defining "Difference": A Metric Showdown

If we were just comparing sets, this would be trivial. But a "Top 100" list is ranked data. Position matters. A metric that treats a swap between \#1 and \#2 the same as a swap between \#99 and \#100 is useless for this context.

I implemented a `ListSimilarity` class to run these lists through a gauntlet of five distinct metrics.

### 1\. Jaccard Similarity: The "Bag of Songs" Approach

The most intuitive place to start is simple set overlap. We ignore the order entirely and look at the intersection over the union.

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

```python
def jaccard_similarity(self):
    set1, set2 = set(self.list1), set(self.list2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union != 0 else 0
```

**The Verdict:** While useful for a quick sanity check, Jaccard is too blunt. If Spotify lists my \#1 song as \#100, Jaccard says "Perfect match." My ears disagree.

### 2\. Spearman Rank Correlation

Stepping up to statistical correlation, Spearman compares the rank values. It essentially assesses the monotonic relationship between the two lists.

```python
def spearman_correlation(self) -> float:
    # ... mapping items to ranks ...
    spear_corr, _ = spearmanr(ranks1, ranks2)
    return spear_corr
```

**The Verdict:** Better, but Spearman tends to be sensitive to outliers. If a song appears in one list but not the other, handling the "missing rank" (imputation) can skew the results significantly depending on how you penalize non-existence.

### 3\. The "Bubble Sort" Distance (Kendall Tau)

This is where it gets interesting. We can visualize the difference between the lists as the amount of "energy" required to transform one into the other. Specifically, what is the minimum number of adjacent swaps (Bubble Sort operations) needed to make the Spotify list look like the Last.fm list?

Mathematically, this corresponds to the **Kendall Tau** distance.

```python
def bubblesort_distance(self) -> float:
    # Align sequences to ensure equal length for comparison
    aligned_seq1 = self.list1 + [x for x in self.list2 if x not in self.list1]
    aligned_seq2 = self.list2 + [x for x in self.list1 if x not in self.list2]
    
    kt_corr, _ = kendalltau(aligned_seq1, aligned_seq2)
    return (1 - kt_corr) / 2
```

**The Verdict:** This feels physically intuitive. It quantifies the "tangledness" of the connection graph. However, it treats all swaps equally. Swapping the top 2 songs should theoretically hurt the score more than swapping \#98 and \#99.

### 4\. Rank-Biased Overlap (RBO)

For recommendation systems and search rankings, the top of the list is valuable real estate. We need a metric that is "top-heavy."

Enter **Rank-Biased Overlap (RBO)**. RBO calculates overlap at incremental depths of the list, weighting the top significantly more based on a persistence parameter $p$.

$$RBO(S, T, p) = (1-p) \sum_{d=1}^{\infty} p^{d-1} \cdot A_d$$

Where $A_d$ is the proportion of overlap at depth $d$.

```python
def rbo(self):
    def overlap_at_depth(depth):
        return len(set(self.list1[:depth]) & set(self.list2[:depth])) / depth

    k = min(len(self.list1), len(self.list2))
    score = 0
    for d in range(1, k + 1):
        overlap = overlap_at_depth(d)
        # Weight decay based on p (set to 0.9)
        score += (1 - self.rbo_p) * (self.rbo_p ** (d - 1)) * overlap
    return score
```

**The Verdict:** This was the most robust metric for the experiment. It acknowledged that while Spotify and Last.fm might disagree on the long tail, they *must* agree on the top 10 to be considered accurate.

## Visualizing the Discrepancy

Metrics are great, but nothing beats a tangled slope graph to induce a headache. I used `matplotlib` (with a toggle for XKCD style, naturally) to draw connections between the ranks.

```python
def connection_graph(list1, list2, top_k=20, ...):
    # ... setup code ...
    for item in all_items:
        if item in positions1 and item in positions2:
            ax.plot([0.03, 0.97], [positions1[item], positions2[item]], "k-", alpha=0.7)
```

The resulting graph usually looks like a handful of spaghetti thrown against a wall. The blue lines (items present in the top K of one but not the other) were particularly revealing. There were tracks in my Spotify Top 20 that I had genuinely zero recollection of obsessed-over, while my actual heavy-rotation tracks were pushed down.

## The Twist: Spotify vs. Spotify

Here is the darker realization.

I assumed the discrepancy was "Spotify vs. Last.fm" (different counting methodologies). But thanks to GDPR, we can request our raw streaming history from Spotify directly.

When I ran the `ListSimilarity` comparison between **Spotify Wrapped** and **Spotify's own raw data**, the results were *not* identical.

The raw logs showed play counts that should have mathematically placed certain songs in the Top 5. Yet, in Wrapped, they were demoted. This confirms a long-held suspicion: **Wrapped is an editorial product, not a statistical query.**

It filters. It likely weights "active listening" (clicking play) higher than "passive listening" (autoplay). It might filter out songs it deems "noise" (like white noise tracks or short loops). The "Top 100" you get is a curated version of your reality.

## Conclusion

So, how much was it off?

Using a composite score of the metrics above (weighted heavily toward RBO and Jaccard), the similarity usually hovers around **60-70%**.

  * **Jaccard** usually punishes the lists for the long tail disagreement.
  * **Kendall Tau** exposes the massive re-ordering that happens in the middle of the pack.
  * **RBO** saves the day by confirming that at least my \#1 song was correct.

As we brace ourselves for the incoming deluge of Wrapped screenshots, keep this in mind: You are looking at a "vibe check," not a database dump. If you want the truth, you have to scrape it yourself.

*(You can find the scripts used for this analysis, including the `ListSimilarity` class and the plotting logic, on my GitHub.)*
